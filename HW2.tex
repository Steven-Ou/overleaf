\documentclass[12pt]{article}

\include{preamble}
%All credit goes towards Dr. Adam Kapelner for providing the preamble and homework template.

\newtoggle{professormode}

\title{MATH 245 Spring \the\year~ Homework \#2}

\author{Steven Ou} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due via Brightspace 11:59PM, March 6, \the\year \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}

\problem{Thus far, we discussed two main ideas which have a lot in common. One idea was modeling with linear regression which we used to illustrate the concepts of cost functions and gradient descent. The other main idea was neural networks which also require a cost function and gradient descent. Before we review neural networks, let us take a step back and analyze the big picture. These next few questions are similar/related to Homework 01 Problem 02.}

\begin{enumerate}

\intermediatesubproblem{ Why do we need training data? How has training data been used in the linear regression case and neural network case that we discussed in class?
}
\\
\\
We need training data because it provides examples from which a model can learn patterns and relationships. Without training data, a model cannot generalize to new inputs. Training data minimizes the cost function in linear regression by adjusting the parameters m (slope) and b (intercept) to best fit the given data points. Training data is also used to optimize the weights and biases of the network layers in neural networks. The goal is to minimize a loss function that allows the network to make accurate predictions on unpredictable data.
\intermediatesubproblem{Suppose given our training data, we selected a model that requires parameters. For instance, in the linear regression case, we required parameters $m$ and $b$ while in the neural network case, we required hundreds of parameters for our weights and biases. Why do we think of these parameters as ``controlling knobs''?}
\\ \\ 
These parameters could be known as "controlling knobs" because they determine how the model transforms input into outputs. Adjusting these parameters changes the behavior of the model. In linear regression, the parameters m and b control the slope and position of the line, affecting how well it fits the training data. In neural networks, the weights and biases in a neural network determine the strength of connections between neurons, determining how features in the input data contribute to the final prediction. By tuning these parameters, we could control the model's ability to capture patterns in data, making them comparable to knobs that adjust performance.   
\vspace{10mm}

\intermediatesubproblem{How do we determine which direction to turn these knobs?}\\
\\
We can determine which direction to turn these knobs so we can adjust parameters using gradient descent, an algorithm that minimizes the cost function by following the negative gradient of the function. We have to compute the gradient by finding the derivative of the cost function concerning each parameter. Then, we update the parameters, adjusting the parameters in the direction that decreases the cost function. And we continued to repeat the progress iteratively until the model converged to a set of best-fit parameters.  \\
\end{enumerate}

\problem{In this question we will review the perceptron.}

\begin{enumerate}

\intermediatesubproblem{Explain what is meant by the perceptron. A good answer would discuss what the inputs and outputs are.}
\\\\ 
Perceptron is a neural network model that takes multiple inputs, it also produces outputs that are binary numbers based on the inputs. \\The input could be represented as a vector $x=x_1,x_2,...,x_n$. The weights of each input $x_i$ are associated with a weight $w_i$. The weight determines the importance of each input. The perceptron also has a bias $b$, which is a threshold for the model to shift the condition of the final choice.\\ The perceptron computes a weighted summation of the inputs, including the bias using the formula: $z = \sum_{i=1}^{n}{w_ix_i +b}$. \\Then using z we would pass it into an activation function, this function would output(0 or 1): if z $\geq$ 0 is not something/choices you wanted to make or do, and z $\leq$1 if the inputs are something/choices you wanted to make or to do. \\This decision-making process allows the machine to use perceptron to differentiate the difference between a good and a bad choice. \vspace{5mm}

\intermediatesubproblem{In the perceptron, what function do we use to compute the output?}
\\\\
We use the activation function to compute the output. The function output is either 0 or 1, depending on whether the weighted sum is positive or negative. Like this function: \[
f(x) =\begin{cases}1 & \text{if } x \geq 0 \\0 & \text{if } x < 0
\end{cases}
\]
\vspace{20mm}

\intermediatesubproblem{Consider the single layer perceptron below.

\begin{figure}[h!]
    \centering
\includegraphics[width=0.75\linewidth]{Neural_Network_1.png}
\end{figure}



Suppose $x_1 = 1, x_2 = 0, x_3 = 1$. Also suppose that $b_{1}^{[1]} = -0.89 $. Compute the perceptron's output. Hint: remember that a perceptron's output is either 0 or 1.
}
\\\\
\centerline{\underline{Weights:} $w_1 =0.1, w_2 =0.4,w_3 =0.8$} \\ 
\centerline{\underline{Inputs:}$x_1 =1, x_2=0,x_3=1$}\\
\centerline{\underline{Bias:}$b_1^{[1]}=-0.89$}\\
\centerline{\underline{Weighted Sum:} $S = (1*0.1)+(0*0.4)+(1*0.8)+(-0.89)$}
\centerline{\underline{Weighted Sum:} $S = 0.1+0+0.8-0.89=0.01 $}\\
\centerline{Since S is greater than and equal to 0, the perceptron output is 1.} 
\vspace{1mm} 
\intermediatesubproblem{In class, we stated that for the perceptron, small changes in the input can produce large changes in the output. Suppose we have the same perceptron as outlined above where $x_1 = 1, x_2 = 0, x_3 = 1$ and this time, $b_{1}^{[1]} = -0.9 $. What is the perceptron's output now?}
\\
\centerline{\underline{Weights:} $w_1 =0.1, w_2 =0.4,w_3 =0.8$} \\ 
\centerline{\underline{Inputs:}$x_1 =1, x_2=0,x_3=1$}\\
\centerline{\underline{Bias:}$b_1^{[1]}=-0.9$}\\
\centerline{\underline{Weighted Sum:} $S = (1*0.1)+(0*0.4)+(1*0.8)+(-0.89)$}\\
\centerline{\underline{Weighted Sum:} $S = 0.1+0+0.8-0.9=0 $}\\
\centerline{Since S is greater than and equal to 0, the perceptron output is still 1.} 

\intermediatesubproblem{In class, we stated that there were two fundamental restrictions with the perceptron. What are these restrictions/problems?}
\\\\
1. The problem is that small changes to our knobs/parameters can yield large changes in output and can only solve problems that cannot be divided by a straight line (not linear).\\
2. The restriction is that the input is rigid since it's either 0 or 1. This makes it hard to differentiate, which prevents the use of a gradient descent algorithm. 
\vspace{5mm}


\intermediatesubproblem{Read the article found here: \url{https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon}. According to the article, what was one of the problems with Roseblatt's perceptron?}
\\\\The problem with Roseblatt's perceptron is that it only has 1 neuron, while modern neural networks have millions of neurons. Since the perceptron only has 1 neuron it was limited to only doing one decision. If it were to have millions of neurons it would be able to model complex choices. But, as Minsky says "the functions are just too simple". This shows the problem with Roseblatt's perceptron because it only has 1 neuron, and it's just too simple and not like today, where a neural network would have millions and millions of neurons. 
\end{enumerate}

\vspace{5mm}


\problem{In the previous question we identified the shortcomings of the perceptron. We now aim to review its upgrade -- the sigmoid neuron.}

\begin{enumerate}
\intermediatesubproblem{How is the sigmoid neuron different from the perceptron? Phrased differently, why is the sigmoid neuron more flexible than the perceptron?}
\\\\ 
The sigmoid neuron is different from perceptron because the sigmoid neuron outputs a value between 0 and 1 using the sigmoid activation function: \\
\centerline{$\sigma(x) = \frac{1}{1+e^-x}$}\\
The sigmoid neuron is more flexible than the perceptron because it makes smooth changes instead of sudden jumps, which helps in learning complex patterns better!

\intermediatesubproblem{Allow us to reconsider the neural network above:

\begin{figure}[h!]
    \centering
\includegraphics[width=0.25\linewidth]{Neural_Network_1.png}
\end{figure}

Suppose the activation function is now the sigmoid function. Compute the output of the sigmoid neuron
when $x_1 = 1, x_2 = 0, x_3 = 1$. (This is the same input as before in Problem 2 part $(c)$).}
\\\\
\centerline{\underline{Weights:} $w_1 =0.1, w_2 =0.4,w_3 =0.8$ }
\centerline{\underline{Wighted Sum Formula:} $S = w_1x_1+w_2x_2+w_3+x_3+b_{1}^{[1]}$}
\centerline{\underline{Weighted Sum:} $S = (1*0.1)+(0*0.4)+(1*0.8)+(-0.89)$}\\
\centerline{\underline{Weight Sum:} $S=0.01$}
\\ \centerline{$\sigma(0.01) = \frac{1}{1+e^{(-0.01)}} \approx 0.5025$} \\ 
\centerline{The output of the sigmoid neuron is 0.5025.}
\vspace{1mm}

\intermediatesubproblem{Allow us to now show that a small change in the weights/bias produces small changes in the output. Suppose we have the same sigmoid neuron as outlined above where $x_1 = 1, x_2 = 0, x_3 = 1$ and this time, $b_{1}^{[1]} = -0.9 $. What is the sigmoid neuron's output now?}
\\\\
\centerline{\underline{Weights:} $w_1 =0.1, w_2 =0.4,w_3 =0.8$ }
\centerline{\underline{Wighted Sum Formula:} $S = w_1x_1+w_2x_2+w_3+x_3+b_{1}^{[1]}$}
\centerline{\underline{Weighted Sum:} $S = (1*0.1)+(0*0.4)+(1*0.8)+(-0.9)$}\\
\centerline{\underline{Weight Sum:} $S=0.00$}
\\ \centerline{$\sigma(0) = \frac{1}{1+e^{(0)}} = 0.5$} \\ 
\centerline{The output of the sigmoid neuron is 0.5.}
\vspace{5mm}

\intermediatesubproblem{What conclusion can you draw regarding small changes in the weights/bias with the perceptron versus the sigmoid neuron?}
\\\\
The small change in the weights/bias with the perception versus the sigmoid neuron is that if the perception has a small change in bias or weights, it can cause the output to flip from 0 to 1. A small change in the weight of a sigmoid neuron would result in little changes in the output. 
\intermediatesubproblem{Compute $a_{1}^{[1]}$ in the neural network below. Assume that $x_1 = 1.2, x_2 = 0.05$, $x_3 = 0.6530$ and use the sigmoid function for the activation function. }
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{NN2.png}
\end{figure}

\vspace{5mm}

\intermediatesubproblem{Compute $a_{2}^{[1]}$. Assume that $x_1 = 1.2, x_2 = 0.05$, $x_3 = 0.6530$ and use the sigmoid function for the activation function.}
\\\\\centerline{\underline{Weights (layer 1 to Hidden Layer 1 and 2):}}\\ \centerline{$w_{11}^{[1]} =0.3 , w_{12}^{[1]} =0.1 , w_{13}^{[1]}=0.5 , w_{21}^{[1]}=0.2 ,w_{22}^{[1]}=0.6 , w_{23}^{[1]} =0.15$} \\
\centerline{\underline{Biases (Hidden Layer 1 and 2):}}\\
\centerline{$b_{1}^{[1]}=0.8 , b_{2}^{[1]} =-1.4$}\\
\centerline{\underline{Weights (Hidden Layer to Output Layer):}}\\
\centerline{$w_{11}^{[2]} =0.2 , w_{12}^{[2]} =0.6$}\\
\centerline{\underline{Bias (Output Layer):}}\\
\centerline{$b_{1}^{[2]}=-1.447$} 
\centerline{\underline{Formula:} $z_{2}^{[1]}=x_1w_{21}^{1}+x_1w_{22}^{1}+x_1w_{23}^{1}+b_{2}^{[1]}$} 
\centerline{$z_{2}^{[1]}=(1.2*0.2)+(0.05*0.6)+(0.6530*0.15)+(-1.4)$}
\centerline{$= 0.24+0.03+0.09795-1.14$}
\centerline{$=-1.03205$}
\centerline{$a_{2}^{[1]} = \sigma(-1.03205)= \frac{1}{1+e^{-1.03205}} \approx 0.2625$}
\centerline{The output of the $a_{2}^{[1]} = 0.2625.$} 
\vspace{5mm}

\intermediatesubproblem{Compute the output of the above network, $y$. Assume that $x_1 = 1.2, x_2 = 0.05$, $x_3 = 0.6530$ and use the sigmoid function for the activation function.}
\\\\
\centerline{$z_{1}^{[1]} = h_1 w_{11}^{[2]} + h_2 w_{12}^{[2]} + b^{[2]}$}
\centerline{$z_{1}^{[1]}=(1.2*0.3)+(0.05*0.1)+(0.6530*0.5)+0.8$}
\centerline{$= 0.36 + 0.005 + 0.3265 + 0.8 = 1.4915$}
\centerline{$a_{1}^{[1]} = \frac{1}{1+e^{-1.4915}} \approx 0.8163$}
\centerline{$z^{[2]}=(0.8163*0.2)+(0.2625*0.6)+(-1.447) = -1.1262$}
\centerline{$y=\frac{1}{1+e^{1.1262}} \approx 0.2448$}
\centerline{ The output of the above network, y is 0.2448.}
\end{enumerate}
\problem{In this question we will review greyscaling.}

\begin{enumerate}
\easysubproblem{What is a pixel?}
\\\\A pixel is the smallest unit of an image, and it represents a single color. 
\vspace{5mm}

\easysubproblem{True or False? The RGB color model states that red, green, and blue can be combined in different ways to produce a wide range of colors.}
\\True. The RGB model mixes red, green, and blue to produce many colors.
\vspace{10mm}


\easysubproblem{What is your favorite color?}
My favorite color is light blue!
\vspace{5mm}

\intermediatesubproblem{Using this site, \url{http://www.cknuckles.com/rgbsliders.html}, find out the RGB values for the color you picked above. Note: that site is not exactly secure but it's the one I used in class. You're welcome to use any other website that has an RGB slider such as this one: \url{https://tuneform.com/tools/color/rgb-color-creator}. }
The RGB values for the color that I picked above are: Red = 0, Green = 226, and Blue = 242.
\vspace{5mm}

\easysubproblem{What formula did we use to convert a color to grayscale? Hint: \url{https://en.wikipedia.org/wiki/Luma_(video)\#Rec._601_luma_versus_Rec._709_luma_coefficients}.}
The formula is called the luminosity method. Y = 0.2989R + 0.5870G+0.1140B.  
\vspace{5mm}

\easysubproblem{Using the formula above, convert the RGB value of your favorite color in part $(c)$ to grayscale. Write down what value of red, green and blue you get.}\\
\centerline{Red = 0, Green = 226, and Blue = 242.}
\centerline{$Y = 0.2989(0) + 0.5870(226)+.1140(242)= 160.25 $}
\centerline{The value is 160.25.}
\vspace{5mm}
\easysubproblem{For the color you get above, what is the grayscale intensity? (Hint: just divide by 255).}\\
\centerline{$\frac{160.25}{255} \approx 0.628$}
The grayscale intensity is approximately 0.628.
\end{enumerate}

\problem{In this question we will review the final product -- our neural network.}

\begin{enumerate}

\easysubproblem{Write down the four steps in creating a neural network.}\\\\
1st: Convert input data like images into numbers and organize them for training datasets. Specifying the input and output layer.
\\\\
\centerline{2nd: Decide the number of layers, neurons per layer, and activation function.}
\\\\
\centerline{3rd: Select our activation and cost function. }
\\\\
4th: Feed in training data and use gradient descent to update weights and biases so the network gets better at making predictions and minimizing the cost function.
\easysubproblem{For the neural network example we considered in class, what was our task?}\\
The network example we considered in class, was used to classify images of the handwriting of the number and recognize patterns within them using a neural network. 
\vspace{5mm}

\easysubproblem{We stated that the inputs were pictures but to be more precise, we needed these inputs to be numerical. How did we get the images to be numerical?}\\\\
We get the images to be numerical by representing them as pixel intensity values. Then normalizing pixel values by dividing by 255 to get values between 0 and 1. Then flatting the image into a 1D array to feed into the neural network.
\vspace{5mm}

\intermediatesubproblem{How many layers did our neural network have?}
\\\\
Our neural network had 3 layers: an input layer, one hidden layer, and an output layer.
\vspace{5mm}

\intermediatesubproblem{Within each layer, how many neurons did we have?}\\\\
 The input layer had 784 neurons for each pixel in the image, hidden layer had 30 neurons, and the output layer had 10 neurons from 0-9.
\vspace{5mm}

\intermediatesubproblem{What do we hope the hidden layer is doing?}
\\\\
The hidden layer helps the network learn important features from the input data to make better predictions.
\intermediatesubproblem{What activation function did we use?}
\\\\
The activation function that we use is the sigmoid function. 
\vspace{5mm}

\intermediatesubproblem{How did we find the weights and biases?}
\\\\
We found the weight and biases by training the network with 60,000 images of handwritten digits, using gradient descent to adjust the parameters. 

\end{enumerate}

\problem{In this question we will run an image through the neural network we saw in class. Please note that you do not have to enter any code; instead, you're just running code that I will provide.}

\begin{enumerate}

\easysubproblem{Download the image on Brightspace called "homework\_image".}

\easysubproblem{Click this link: \\ \url{https://colab.research.google.com/drive/12trE_j0jaPpy92WiGEG5jVV0B6fqPE0x?usp=sharing}. You wouldn't be able to edit this code since this is located in my Google Drive. You'll need to save a copy of this in your own Drive. To do so, click "File" followed by "Save a Copy in Drive". }

\easysubproblem{Execute the first three cells. It should take about 8 minutes. Upon executing the third cell, you will be prompted to upload an image. Upload the image you downloaded from Brightspace called "homework\_image".}

\easysubproblem{Execute the remaining cell blocks and make a note of the probabilities you obtain in the last code block. What does the neural network predict the digit is?}
The neural network predicted the digit to be a 6.
\vspace{5mm}

\easysubproblem{Redo parts $(c)$ and $ (d)$ again. Did you get a different result or different probabilities? }
I got an eight after doing parts c and d again. The results were different, and the probabilities were also different.
\vspace{5mm}

\extracreditsubproblem{Above, you should get slightly different results each time. Can you perhaps explain why? }\\\\
The results each time can get slightly different because the weights and biases are typically initialized randomly. The training could also use stochastic gradient descent or mini-batch gradient descent, and the updates to the weight and biases are made using random subsets of the training data. The neural network is also sensitive to small differences in input values. Which can lead to slightly different predictions. The computers store numbers with limited precision, which can cause the computer to mess up when running the same neural network multiple times.    
\end{enumerate}

\problem{Here are two unrelated questions.}

\begin{enumerate}

\easysubproblem{Consider this link: \\ \url{https://machinelearning.apple.com/research/learning-neural-network-subspaces}. What kind of neural network does Apple use for features such as face recognition?}
\\\\
Apple uses a deep convolutional neural network with deep feature learning and subspace learning techniques for Face ID and another recognition task.
\vspace{5mm}

\easysubproblem{Is there a particular topic in modeling that you are interested in or would like to learn more about? Examples: Spotify recommendation algorithms, sports betting, transformers like ChatGPT. If no, then you may leave this question blank.}

\end{enumerate}


\end{document}
