\documentclass[12pt]{article}

% --- It's crucial that 'preamble.tex' ONLY contains \usepackage, \newcommand, etc. ---
% --- and NO \documentclass, \begin{document}, or \end{document} commands. ---
\input{preamble.tex} % Make sure 'preamble.tex' is in the same directory

% --- Packages for Enhanced Styling (Load AFTER preamble.tex) ---
\usepackage[dvipsnames]{xcolor} % For more color names and \definecolor.
% \usepackage{titlesec} % TITLESCEC REMAINS COMMENTED OUT TO AVOID THE ERROR
\usepackage{enumitem}     % For customizing list environments
\usepackage[most]{tcolorbox}
\usepackage{etoolbox}     % For \newtoggle and \iftoggle
\usepackage{float}
\usepackage{xurl}
% \usepackage{dsfont} % Likely in preamble
\usepackage{systeme}
% \usepackage{esvect} % Likely in preamble
% \usepackage{amsmath} % Likely in preamble
% \usepackage{amssymb} % Likely in preamble
% \usepackage{amsfonts} % Likely in preamble
\usepackage{bm}           % For \boldsymbol

% --- Custom Color Definitions ---
\definecolor{PrimaryBlue}{RGB}{23, 100, 190}
\definecolor{LightBlueBg}{RGB}{230, 237, 245}
\definecolor{DarkText}{RGB}{50, 50, 50}
\definecolor{BoxFrameBlue}{RGB}{100, 150, 220}
\definecolor{TitleGray}{RGB}{70, 70, 70}

% --- Text Color ---
\color{DarkText}

% --- Vector Command (user-defined) ---
\newcommand{\vect}[1]{\boldsymbol{#1}}

% --- Toggle for Professor Mode ---
\newtoggle{professormode}
\togglefalse{professormode} % Default to false
% \toggletrue{professormode} % Uncomment to enable

% --- Title Styling ---
\title{\textcolor{PrimaryBlue}{\textsc{MATH 245 Spring \the\year~ Homework 4}}}
\author{\textcolor{TitleGray}{Steven Ou}}

\iftoggle{professormode}{
    \date{Due via Brightspace Monday, May $19^{th}$, \the\year ~ at 11:59PM\\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}{
    \date{\textcolor{TitleGray}{\today}}
}

\renewcommand{\abstractname}{\textcolor{PrimaryBlue}{Instructions and Philosophy}}

% --- Commands for Question Styling ---
\newcommand{\easyquestion}[1]{\item \textcolor{PrimaryBlue}{#1~{\normalfont\itshape\small (easy)}}}
\newcommand{\intermediatequestion}[1]{\item \textcolor{PrimaryBlue}{#1~{\normalfont\itshape\small (intermediate)}}}
% If you have questions that are neither easy nor intermediate, you could use:
% \newcommand{\regularquestion}[1]{\item \textcolor{PrimaryBlue}{#1}}

% --- List Item Styling (for sub-problems: a), b), c), etc.) ---
% This now makes the first level of enumeration (sub-problems) a), b), c) in blue and bold.
\setlist[enumerate,1]{label=\textcolor{PrimaryBlue}{\alph*).}, font=\bfseries, itemsep=0.5ex, topsep=0.5ex}
\setlist[enumerate,2]{label=\textcolor{PrimaryBlue}{\roman*).}, font=\bfseries, itemsep=0.5ex, topsep=0.5ex} % For potential sub-sub-problems (i, ii, etc.)

% --- tcolorbox default styling ---
\tcbset{
    colback=LightBlueBg,
    colframe=BoxFrameBlue,
    coltext=DarkText,
    fonttitle=\bfseries,
    coltitle=white,
    colbacktitle=PrimaryBlue,
    arc=3mm,
    boxrule=0.8pt,
    drop fuzzy shadow=PrimaryBlue!30!white,
    breakable,
}


\begin{document}
\maketitle
\iftoggle{professormode}{
    \begin{abstract}
    % Abstract content for professor mode if any
    \end{abstract}

    \thispagestyle{empty}
    \vspace{1cm}
    \textcolor{DarkText}{NAME: \line(1,0){380}}
    \clearpage
}{}

\section*{Problem 1} % Main problems are "Problem 1", "Problem 2", etc.
In this question, we are going to do some light reading. We will only read selected selections of the following article: \url{https://www.nature.com/articles/s41598-024-51600-y.pdf}. You do NOT have to read the entire article.

\begin{enumerate} % Sub-problems will now be a), b), c)...
\easyquestion{What year was this paper published?}
\\\\This paper was published in 2024.
\easyquestion{In the article, what task are the researchers attempting to accomplish? Hint: look at the title.}
\\\\ Detecting Parkinson's disease through L1 regularized SVM and deep neural network.
\easyquestion{Which two machine learning algorithms/models are being used to accomplish their task? Hint: again look at the title.}
\\\\SVM and neural network
\easyquestion{True or False? In previous studies, the detection of Parkinson's disease had rather low accuracy.}
\\\\ True!
\easyquestion{What is Parkinson's disease?}
\\\\
 Parkinson's disease is a progressive neurological disorder that affects movement. It is caused by the degeneration of dopamine-producing neurons in the brain and typically leads to tremors, stiffness, slowness of movement, and balance difficulties.
\easyquestion{What is the typical training data for the detection of Parkinson's disease? Hint: see the last paragraph of Page 01.}
\\\\Voice recordings and speech signals are commonly used as training data because Parkinsonâ€™s can cause vocal impairments.
\easyquestion{What two stage diagnostic model are the researchers proposing? Hint: see the fourth paragraph on Page 02.}
\\\\The first diagnostic model is $l_1$, which regularizes SVM and selects important features. The next model is DNN, which performs a classification based on the selected features.
\easyquestion{How many datasets were used? Hint: see Page 03.}
\\\\Two datasets were used.
\easyquestion{With regards to dataset 1, how many hyperparameters were used? Hint: see page 07.}
\\\\4 hyperparameters were tuned for dataset 1.
\easyquestion{With regards to dataset 2, how many hyperparameters were used? Hint: see page 07.}
\\\\3 hyperparameters were tuned for dataset 2.
\easyquestion{With regards to dataset 01, what was the accuracy? Hint: see Table 10.}
\\\\From table 10, the accuracy for dataset 01 was $95.56\%$.
\easyquestion{With regards to dataset 02, what was the accuracy? Hint: see Table 11.}
\\\\From table 11, the accuracy for dataset 02 was 99.17$\%$.
\easyquestion{Write down one limitation of this study. Hint: see page 10.}
\\\\ One limitation is that the data sets used were relatively small, which can limit the generalizability of the results to larger populations.
\intermediatequestion{Explain why classifying a patient as either having PD or not having PD is a worthwhile goal. (The answer to this isn't necessarily found in the paper but rather based on your own reasoning).}
\\\\The earlier you find out that you have PD, you can receive prompt treatment, improve your quality of life, and slow the progression of symptoms. It can also help reduce the burden on healthcare systems by enabling earlier intervention. \\\\
\end{enumerate}
\newpage
\section*{Problem 2}
In this exercise, we will review the basics needed in developing the support vector machine.

\begin{enumerate} % Sub-problems will now be a), b), c)...
\easyquestion{Suppose we are given a bunch of data points which belong to one of two classes and these points are labeled (meaning we see that each data point has a label of either $+1$ or $-1$). What is our goal/task?}
\\\\
Our goal is to learn a decision boundary that correctly separates the data points into their respective classes (+1 or -1), ensuring that they have many spaces between them.

\easyquestion{What does our training data $\mathds{D}$ typically look like?}
\\\\This $\mathds{D}$ should look like:
$\mathds{D} = \{(\vect{x_1},y_1),(\vect{x_2},y_2),...,(\vect{x_n},y_n)\}$ ,where $\vect{x_i}\in \mathbb{R}^d$ and $ y_i \in\{-1,+1\}$.

\easyquestion{What does it mean when we say that data is linearly separable?}
\\\\Data is linearly separable if there exists a straight line (in 2D) or a hyperplane (in higher dimensions) that perfectly separates the two classes with no misclassifications.

\easyquestion{If the data is linearly separable, then how many hyperplanes can we typically draw which separates the data and correctly classifies everything?}
\\\\Infinitely many hyperplanes can correctly separate linearly separable data.

\easyquestion{In the space below, sketch out a linearly separable dataset.}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{1.png}
    % \caption{A linearly separable dataset.}
\end{figure}

\easyquestion{Again sketch out your linearly separable dataset above and this time draw a hyperplane which divides the space and ensures correct classification.}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{2.png}
    % \caption{A linearly separable dataset.}
\end{figure}

\easyquestion{Again, copy your linearly separable dataset from part $(e)$ and this time draw a hyperplane different from your answer in part $(f)$  which divides the space and ensures correct classification.}
% Students would draw on the image or provide a new one. (Assumed 3.png)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{3.png} % Assuming you name the third image 3.png
    % \caption{A linearly separable dataset with a different hyperplane.}
\end{figure}


\easyquestion{Given your answer in the above two parts, we now have two hyperplanes which correctly separates and classifies. Between your two hyperplanes, which hyperplane do you think is better? Why?}
% Student answer space

\easyquestion{Intuitively explain what is meant by \textit{margin}.}
% Student answer space

\easyquestion{Mathematically, how do we define \textit{margin}?}
% Student answer space

\easyquestion{With regards to the margin, what is our goal?}
% Student answer space

\easyquestion{If our \textbf{only} goal is to maximize the margin, explain why our optimization fails.}
% Student answer space

\easyquestion{Explain why we need the constraint $\forall i, y_i(\vect{w}^T \vect{x_i} + b) \geq 0$.}
% Student answer space
\end{enumerate}

\section*{Problem 3}
In this exercise, we will now talk about the equivalent formulations of our hard margin SVM.
\begin{enumerate} % Sub-problems will now be a), b), c)...

\easyquestion{Explain why
\begin{tcolorbox}
    \begin{align*}
     \underset{\vect{w}, b}{\text{arg max}} ~~ \gamma(\vect{w}, b) ~~ \text{subject to the condition} ~ \forall i, y_i(\vect{w}^T \vect{x_i} + b) \geq 0
\end{align*}
\end{tcolorbox}
is equivalent to
\begin{tcolorbox}
    \begin{align*}
     \underset{\vect{w}, b}{\text{arg min}} ~~ \vect{w}^T \vect{w} ~~ \text{subject to the condition} ~& \forall i, y_i(\vect{w}^T \vect{x_i} + b) \geq 0 ~ \text{and} \\ &\underset{\vect{x_i} \in \mathds{D} }{\text{min}} |\vect{w}^T \vect{x_i} + b| = 1
\end{align*}
\end{tcolorbox}}

\newpage

\easyquestion{Explain if the following optimization
\begin{tcolorbox}
\begin{align*}
     \underset{\vect{w}, b}{\text{arg min}} ~~ \vect{w}^T \vect{w} ~~ \text{subject to the condition} ~& \forall i, y_i(\vect{w}^T \vect{x_i} + b) \geq 0 ~ \text{and} \\ &\underset{\vect{x_i} \in \mathds{D} }{\text{min}} |\vect{w}^T \vect{x_i} + b| = 1.
\end{align*}
\end{tcolorbox}
is equivalent to this optimization:
\begin{tcolorbox}
\begin{align*}
     \underset{\vect{w}, b}{\text{arg min}} ~~ \frac{1}{2} \vect{w}^T \vect{w} ~~ \text{subject to the condition} ~& \forall i, y_i(\vect{w}^T \vect{x_i} + b) \geq 0 ~ \text{and} \\ &\underset{\vect{x_i} \in \mathds{D} }{\text{min}} |\vect{w}^T \vect{x_i} + b| = 1
\end{align*}
\end{tcolorbox} }

\vspace{40mm}

\easyquestion{State the primal form (of the hard margin) SVM.}

\newpage

\intermediatequestion{Explain why
\begin{tcolorbox}
\begin{align*}
     \underset{\vect{w}, b}{\text{arg min}} ~~ \vect{w}^T \vect{w} ~~ \text{subject to the condition} ~& \forall i, y_i(\vect{w}^T \vect{x_i} + b) \geq 0 ~ \text{and} \\ &\underset{\vect{x_i} \in \mathds{D} }{\text{min}} |\vect{w}^T \vect{x_i} + b| = 1.
\end{align*}
\end{tcolorbox}
implies the primal form (of the hard margin) SVM. (You do NOT have to show the primal form implies the boxed in optimization which is slightly harder).}

\vspace{60mm}

\easyquestion{Interpret what the primal form (of the hard margin) SVM is asking us to do graphically.}

\vspace{60mm}

\easyquestion{What technique do we use to solve the optimization problem posed in the primal form?}

\newpage

\intermediatequestion{Suppose we are given
$\vect{x_1} = [1 ~~ 0]^T$ with label $-1$,
$\vect{x_2} = [2 ~~ 0]^T$ with label $-1$, and
$\vect{x_3} = [5 ~~ 5]^T$ with label 1. Perform the first two iterations of the subgradient approach where we initialized at $\vect{w} = [1 ~~~ 1]^T$, $b = 0$ and $\alpha = 0.001$.}

\newpage

\easyquestion{Suppose the optimal hyperplane for a  dataset $\mathds{D}$ is given by $\frac{3}{17}x_1 + \frac{5}{17} x_2 - \frac{23}{17} = 0$. Predict the label for the new datapoint $\vect{x}_{\text{new}} = [4 ~~
 2.5]^T$}

\end{enumerate}

\vspace{30mm}
\section*{Problem 4}
In this exercise, we will now talk about the soft margin SVM.
\begin{enumerate} % Sub-problems will now be a), b), c)...

\easyquestion{Explain why in the case of non-linearly separable data the primal form of the hard margin SVM is impossible to satisfy.}

\vspace{30mm}

\easyquestion{In the case of the soft margin SVM, what is the optimization problem we aim to solve?}

\vspace{50mm}

\easyquestion{In the case of the soft margin SVM, what does $\xi_i$ represent?}

\newpage
\intermediatequestion{In the graph below, a particular point has been circled in green. For that point, sketch  where $\xi_i$ is. }
\begin{figure}[H]
    \centering
\includegraphics[width=1\linewidth]{svm_1.png}
\end{figure}

\newpage
\intermediatequestion{In the graph below, a particular point has been circled in green. For that point, sketch  where $\xi_i$ is. }
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{svm_2.png}
\end{figure}

\newpage

\intermediatequestion{In both graphs below, the soft margin SVM has been implemented. One of these graphs correspond to when $C = 0.01$ and the other graph corresponds to when $C = 100$. Which graph is which? How do you know?
\begin{figure}[H]
    \centering
\includegraphics[width=0.75\linewidth]{svm_4.png}
\end{figure}
}
\end{enumerate}

\newpage

\section*{Problem 5}
Here are some additional questions that we discussed during our derivation of the SVM.
\begin{enumerate} % Sub-problems will now be a), b), c)...

\easyquestion{What is the Blessing of Dimensionality? Give an example.}

\vspace{75mm}

\easyquestion{What is the Blessing of Non-Uniformity?}

\vspace{75mm}

\intermediatequestion{Suppose the $x$-axis represents tumor size and the $y-axis$ represents the cell density like the example discussed in class. If getting cancer was indeed random, then what would the graph look like?}
% Student answer space

\end{enumerate}

\end{document}
