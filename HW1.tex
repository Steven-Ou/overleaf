\documentclass[12pt]{article}

\include{preamble}
%All credit goes towards Dr. Adam Kapelner for providing the preamble and homework template.

\newtoggle{professormode}




\title{MATH 245 Spring \the\year~ Homework \#1}

\author{Steven Ou} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due via Brightspace 11:59PM, February 20, \the\year \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

Bonus points are given as a bonus if the homework is typed using \LaTeX. Links to installing \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{https://www.overleaf.com/}. If you are handing in homework this way, read the comments in the code; there is a line to comment out and you should replace my name with yours. The easiest way to use Overleaf is to go to \textit{Menu} and select \textit{Copy Project}. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.


\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME:  \line(1,0){380}
\clearpage
}

\problem{In this exercise, we will review some general ideas about modeling. }

\begin{enumerate}

\easysubproblem{ What is a \textit{model}?
}
\vspace{5mm}
\newline \newline A model is just an approximation to reality. 
\vspace{5mm}

\intermediatesubproblem{Give an example of a model (preferably one that was not discussed in class) and explain why this is a model.}
\vspace{5mm}
\newline \newline An example of a model that we did not discuss in class is a cart, which models a real-life car. Even though it doesn't perfectly simulate a car, a cart is considered a model since we can use it as a reference to an actual car. We can also use simulations to make predictions about certain aspects of actual cars, such as their speed.
\vspace{5mm}

\intermediatesubproblem{George Box stated that \qu{All models are wrong \ldots}. If all models are wrong, then why do we study models?}
\vspace{5mm}
\newline \newline Even if all models are wrong, we can still make predictions, test hypotheses, and improve decision-making, even though they are never perfect. Models can evolve and become more accurate and useful for scientific and experimental use. 

\vspace{5mm}
\easysubproblem{ What are the two goals of modeling? }
\newline \newline The two goals of modeling are prediction and explanation.  
\vspace{10mm}

\easysubproblem{Suppose there is some phenomenon that we wish to model. Do we usually know all of the inputs or drivers of this phenomenon? }
\newline \newline No, we don't know all the inputs or drivers of this phenomenon, because they may be difficult or impossible to measure accurately.
\vspace{5mm}

\easysubproblem{In class, we stated that finding a function or a model can be difficult and that recent advancements in artificial intelligence, particularly machine learning, can help us. What is artificial intelligence? }
\newline \newline Artificial intelligence is a term that refers to the general ability of computers to emulate human thoughts and use human intelligence to perform tasks. 
\vspace{5mm}

\easysubproblem{In class, we stated that finding a function or a model can be difficult and that recent advancements in artificial intelligence, particularly machine learning, can help us. What is machine learning? (Use the most recent definition we discussed in class). } 
\newline \newline
Machine learning is used to train a piece of software to make useful predictions to generate content from data and to learn and improve their performance over time. 
\vspace{5mm} 

\easysubproblem{Read the section called \textit{History} on this page: \url{https://en.wikipedia.org/wiki/Machine_learning}. What was one of the first machine learning models?}
\newline \newline 
The first machine learning model is called \textcolor{red}{'Computer Checkers'}. 
\vspace{5mm}

\easysubproblem{Read the section called \textit{History} on this page: \url{https://en.wikipedia.org/wiki/Machine_learning}. How did Tom Mitchell define machine learning?}
\newline \newline 
Tom Mitchell defines machine learning by saying that the program performance \textcolor{purple} {(p)} on task \textcolor{orange} {(t)} as measured by \textcolor{purple} {p}, improves with experience \textcolor{green}{(e)}. This also means that the machine is like a baby and needs to be taught step by step as the baby goes through its task. And improves and learns every time the baby makes mistakes.
\vspace{5mm}

\easysubproblem{True or False? All of artificial intelligence falls under machine learning.}
\newline \newline 
False!
\vspace{5mm}

\easysubproblem{True or False? All of machine learning falls under artificial intelligence.}
\newline \newline
True!

\end{enumerate}


\problem{These exercises will review the need for gradient descent.}


\begin{enumerate}

\easysubproblem{Suppose we have a bunch of data points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n),$ and we wish to use a linear model to predict the value of $y$. What parameters (or values) are we looking to find?}
\newline \newline 
The parameters that we're looking to find are the slope $(m)$ and the intercept $(b)$ in the equation $\hat{y}=mx+b.$
\vspace{5mm}

\easysubproblem{In order to find these parameters, we have to define a cost function. How many dimensions would be required to graph the cost function?}
\newline \newline 
Graphing the cost function would require at least three dimensions because it depends on $m$ and $b$ and the cost value (c). 

\vspace{5mm}

\easysubproblem{Once we have our cost function, what exactly are we looking to do with this function? What is our primary objective?}
\newline \newline
What we want to do with the cost function is find the values of $m$ and $b$ that yield the lowest cost. Our primary objective is to obtain the best-fitting model. 
\vspace{5mm}

\easysubproblem{Explain how a machine can use the cost function to learn better values for our parameters. A full answer would outline the algorithm being used for the computer/machine to learn.}
\newline \newline 
A machine can use the cost function to learn better values for the parameters by repeatedly modifying $m$ and $b$ using an algorithm called gradient descent. It calculates the gradient of the cost function with the parameters and updates it in the direction that shows the cost decreasing until it converges to an acceptable solution. 

\vspace{5mm}


\intermediatesubproblem{Using your answer above, explain what it means when we say the machine is learning?}
\newline \newline
When we say the machine is learning, we mean that it is repeatedly testing the model parameters based on the cost function to minimize error. The machine would learn as it updates its understanding of the data through gradient descent and it will improve its predictions over time. 

\vspace{20mm}


\intermediatesubproblem{Now suppose we have a bunch of data points $(x_1, y_1), (x_2, y_2), \ldots (x_n, y_n),$ and we wish to use a quadratic model to predict the value of $y$. That is, we wish to find real numbers $a$, $b$, and $c$ such that $\hat{y_i} = a{x_i}^2 + b{x_i} + c.$ How many dimensions would be required to plot the cost function? Can we actually graph this function? If not, then is there a particular technique we can use?}
\newline \newline 
It would take 4 dimensions to plot the cost function because the cost function would depend on the three parameters (a,b, and c). We can't directly graph this, but we can use gradient descent to graph this function. 

\vspace{5mm}


\end{enumerate}

\problem{In class, we performed the gradient descent algorithm on $f(x) = (x-5)^2$ by starting at $x=0$ and using the learning rate, $\alpha = 0.1$.}


\begin{enumerate}

\intermediatesubproblem{This time, perform the next three iterations on $f(x) = (x-5)^2$ by starting at $x=0$ and using the learning rate, $\alpha = 4$. Describe what happens.}
\newline \newline
\textcolor{blue}{\underline{STEP 1:}}
\newline
$x = 5$, assigning a random value to $x$
\newline 
$f(5) = (5-5)^2 = 0$, calculate $f(x)$
\newline
$f'(5) = 2(5-5)(1) = 0$, calculating $f'(x)$
\newline
$\alpha = 4$, defining the learning rate
\newline \newline
\textcolor{blue}{\underline{STEP 2: Iteration 1, 2, 3}}
\newline
Using the formula $x_{new} = x_{old}- \alpha f'(x_{old})$
\newline \newline
Starting with $x_{0} = 0$
\newline
$f'(0) = 2(0-5) = -10$
\newline 
$x_{1} = 0-4(-10)= 40$
\newline \newline  
Starting with $x_{1} = 40$
\newline
$f'(40) = 2(40-5) = 70$
\newline 
$x_{2} = 40-4(70)= -240$
\newline \newline  
Starting with $x_{2} = -240$
\newline
$f'(-240) = 2(-240-5) = -490$
\newline 
$x_{3} = -240-4(-490) = 1720$
\newline \newline
Based on the three iterations on $f(x) = (x-5)^2$  with $x = 0$ and the learning rate as $\alpha =4$ it is too large, causing the updates to overshoot or undershoot the minimum rather than steadily converging to it. This shows that the learning rate needs to be carefully chosen, for the success gradient descent. 
\newpage

\end{enumerate}

\problem{In this exercise, we will practice a one dimensional gradient descent. For all of the parts below, let $f(x) = x^4 + x^3 - 2x^2$.}


\begin{enumerate}

\easysubproblem{Sketch a graph of $f(x)$.}

\begin{center}
\includegraphics[width=0.40\linewidth]{image1.png}
\end{center}

\vspace{5mm}

\intermediatesubproblem{Graphically, where is the global minimum of the function? If needed, round your answer to two decimal places.}
\newline
Step 1: Take the derivative of $f(x)$: \\
\\
$f'(x) = 4x^3 + 3x^2 - 4x$ \\
\\
Step 2: Solve for all values of $x$ such that $f'(x) = 0$: \\
\\
$4x^3 + 3x^2 - 4x = 0$ \\
\\
$x(4x^2 + 3x -4) = 0$ \\
\\
$x = \{0, \frac{-3 - \sqrt{73}}{8}, \frac{-3 + \sqrt{73}}{8}\}$ \\
\\
Step 3: Determine which value of $x$ corresponds to the minimum of $f(x)$: \\
\\
$f(0) = 0 $ \\
\\
$f(\frac{-3 - \sqrt{73}}{8}) \approx -2.83$ \\
\\
$f(\frac{-3 + \sqrt{73}}{8}) \approx -0.4$ \\
Since $-2.83$ is the smallest of the three values, we conclude that $x = \frac{-3 - \sqrt{73}}{8} \approx -1.44$ is the $x$ value that corresponds to the global minimum value of $f(x)$.

\newpage


\intermediatesubproblem{Now suppose we wish to find the minimum by using gradient descent. Starting at $x=1$, compute what happens over the next three iterations. You may take the learning rate to be $\alpha = 0.1$. }
\\
\textcolor{blue}{\underline{STEP 1:}}
\\
$f(x) = x^4+x^3 -2x^2$
\newline
$x = 1$ assigning a random value to $x$
\newline 
$f(1) = (1)+(1)-2= 0$, calculate $f(x)$
\newline
$f'(x) = 4x^3+3x^2-4x$
\\ 
$f'(5) = 4(5)^3+3(5)^2-4(5) = 555$, calculating $f'(x)$
\newline
$\alpha = 0.1$, defining $\alpha$
\newline \newline
\textcolor{blue}{\underline{STEP 2: Iteration 1, 2, 3}}
\newline
Using the formula $x_{new} = x_{old}- \alpha f'(x_{old})$
\newline \newline
Starting with $x_{0} = 1$
\newline
$f'(1) =4(1)+3(1)-4= 3$
\newline 
$x_{1} =1-(0.1 \cdot 3)= 0.7$
\newline \newline  
Starting with $x_{1} = 0.7$
\newline
$f'(0.7) = 4(0.7)^3+3(0.7)^2-4(0.7)= 0.042$
\newline 
$x_{2} = 0.7 - (0.1 \cdot 0.042) = 0.6958$
\newline \newline  
Starting with $x_{2} = 0.6958$
\newline
$f'(0.6958) = 4(0.6958)^3+3(0.6958)^2-4(0.6958) \approx 0.0167$
\newline 
$x_{3} = 0.6958 - 0.0167 = 0.6791$
\newline \newline

Using gradient descent with the starting value of $x = 1$ and a learning rate of $0.1$, the minimum is $x \approx 0.6791$. 

\newpage
\extracreditsubproblem{This optional question is marked extra credit since we have yet to implement this in class. Compute what happens after 500 iterations. You may use a calculator, Excel, or a programming language. Please explain what you did to get your answer. }
\\\\

\begin{center}
\includegraphics[width=1\linewidth]{image2.png}
\end{center}

This program takes the gradient of the function and tries to find the minimum of the function with a starting point, a learning rate, and the maximum number of iterations, and get the convergence. In the for loop, gradient descent was applied, and the for loop calculated the change in x. Then, we check if the change in x is similar to the last change in x. If it is, we can assume that the function has looped to the point where it's close enough to a minimum and the loop would break. The function g(x) is the derivative of the original function $x^4+x^3-2x^2$. The code finally runs the last function which is gradientDescentAlgorithm() and should have some parameters in it like the g(x), the starting point, the learning rate, and the iteration. This is how I used the program to find the minimum of the function. 
\vspace{5mm}

\intermediatesubproblem{If you had to guess, what value do you think our algorithm in Problem 4 Part $(c)$ will converge to?}
\\\\
I would've guessed the value would've been 0.680 because that is around the minimum in the graph that I sketched above.
\vspace{5mm}

\intermediatesubproblem{Now suppose we change our starting point to  $x= -1$. Compute what happens over the next three iterations. You may take the learning rate to be $\alpha = 0.1$. }
\\\\
\textcolor{blue}{\underline{STEP 1:}}
\\ $
f(x) = x^4+x^3 -2x^2 $ //the 
 function
 \\
 $x=-1$ //starting with -1
 \\
 $\alpha = 0.1$\\
$f'(x) = 4x^3+3x^2-4x$ //finding the  derivative
\\
$x_{new} = x_{old} - \alpha f'(x_{old})$ //gradient descent formula: \\
\textcolor{blue}{\underline{STEP 2:}}
\\\\
{\underline{Iteration 1:}
\\
$x_0 = -1$ \\
$f'(-1) = 4(-1)^3 + 3(-1)^2 - 4(-1) = 3$ \\ 
$x_1 = -1 - 0.1(3)= -1.3$\\
{\underline{Iteration 2:}\\
$x_1 = -1.3$ \\ 
$f'(-1.3) = 4(-1.3)^3 + 3(-1.3)^2 - 4(-1.3)= 1.482$ \\ 
$x_2 = -1.3 - 0.1(1.482) = -1.4482$\\ 
{\underline{Iteration 3:}\\
$x_2 = -1.4482$ \\ 
$f'(-1.4482) = 4(-1.4482)^3 + 3(-1.4482)^2 - 4(-1.4482) \approx  -0.046$ \\
$x_3 = -1.4482 - 0.1(-0.046) = -1.4436$\\

As the iterations continue, the change of x of each iteration would start to become closer and closer to the minimum. 

\vspace{5mm}
\intermediatesubproblem{*If you had to guess, what value do you think our algorithm in Problem 4 Part $(f)$ will converge to?}
\\\\
I would have guessed that the value of our algorithm would've converged to approximately -1.44. 
\vspace{5mm}

\intermediatesubproblem{*Do you think that in using gradient descent the starting point matters?}
\\\\
Yes, the starting point in gradient descent matters a lot. As shown in the previous parts of the problem, different starting points can lead to convergence to different local minima. In part c, starting at x =1, the algorithm converted to a local min around 0.693. In part f, starting with x = -1, the algorithm converged to a different local min of about -1.44. This is because gradient descent is a local optimization algorithm. The graph shows its direction would lead to the steepest descent from the current point. If the function has multiple local minima, the algorithm will converge to the minimum that is "closest" to the starting point. It will not necessarily find the global minimum. Therefore, the starting point is really important since it determines which local minimum the algorithm will converge to. 
\end{enumerate}


\end{document}
