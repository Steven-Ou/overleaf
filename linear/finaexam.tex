\documentclass[9pt,landscape,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.3in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, bm, enumitem, xcolor, tcolorbox, multicol}

% Custom Box Styles for High Density and Visual Clarity
\tcbset{
    enhanced, boxrule=0.5pt, arc=0pt, outer arc=0pt,
    fonttitle=\bfseries\sffamily, breakable,
    left=2pt, right=2pt, top=6pt, bottom=2pt
}
\newtcolorbox{algobox}[1]{colback=blue!5!white, colframe=blue!75!black, title={#1}}
\newtcolorbox{exbox}[1]{colback=green!5!white, colframe=green!75!black, title={#1}}
\newtcolorbox{thmbox}[1]{colback=red!5!white, colframe=red!75!black, title={#1}}

\setlist{nosep}

\begin{document}

% --- PAGE 1: SYSTEMS, MATRIX ALGEBRA & INVERTIBILITY ---
\begin{center}
    \textbf{\Large MATH 231 FINAL EXAM BATTLE PLAN - PAGE 1} \\
    \textit{Linear Systems, Augmented Matrices, and the Master IMT}
\end{center}

\begin{multicols}{2}
\section*{1. Linear Systems \& RREF Procedures (Sec 1.1, 1.2)}
\begin{algobox}{RREF and Solution Counts}
\begin{itemize}
    \item \textbf{Augmented Matrix}: $[A \mid b]$.
    \item \textbf{Pivot}: Leading 1 in RREF. \textbf{Basic}: Pivot cols; \textbf{Free}: Non-pivot.
    \item \textbf{Consistency}: Inconsistent if the last column of $[A \mid b]$ is a pivot.
    \item \textbf{Unique}: Pivot in every column of $A$; \textbf{Infinite}: Consistent with free variables.
\end{itemize}
\end{algobox}

\begin{exbox}{Example: Parametric Solution}
$rref(A) = \begin{bmatrix} 1 & -7 & 0 & 2 \\ 0 & 0 & 1 & -1 \\ 0 & 0 & 0 & 0 \end{bmatrix}$. Basic: $x_1, x_3$. Free: $x_2, x_4$.
$x_1 = 7x_2 - 2x_4$ and $x_3 = x_4$.
\end{exbox}

\section*{2. The Invertible Matrix Theorem (IMT) (Sec 2.3)}
\begin{thmbox}{Equivalent Statements for $n \times n$ Matrix $A$}
\begin{itemize}
    \item $A$ is invertible $\iff \det(A) \neq 0$.
    \item $A \sim I_n$ and has $n$ pivot positions.
    \item $Ax = 0$ has only the trivial solution ($x=0$).
    \item Columns form a basis for $\mathbb{R}^n$ and are linearly independent.
    \item $x \mapsto Ax$ is one-to-one and onto $\mathbb{R}^n$.
    \item $rank(A) = n$ and $nullity(A) = 0$.
    \item $0$ is NOT an eigenvalue.
\end{itemize}
\end{thmbox}

\section*{3. Matrix Algebra \& Inverse Computation (Sec 2.1, 2.2)}
\begin{algobox}{Inverse Properties and Algorithms}
\begin{itemize}
    \item \textbf{Inverses}: $(AB)^{-1} = B^{-1}A^{-1}$.
    \item \textbf{Algorithm}: $[A \mid I] \sim [I \mid A^{-1}]$.
    \item \textbf{2x2 Shortcut}: $A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$.
    \item \textbf{Logic}: If $AB$ and $B$ are invertible, $A = (AB)B^{-1}$ is invertible.
\end{itemize}
\end{algobox}
\end{multicols}

\newpage
% --- PAGE 2: DETERMINANTS & TRANSFORMATIONS ---
\begin{center}
    \textbf{\Large MATH 231 FINAL EXAM BATTLE PLAN - PAGE 2} \\
    \textit{Determinant Algebra, Transformations, and Vlamis Logic Arguments}
\end{center}

\begin{multicols}{2}
\section*{4. Determinant Operations (Sec 3.1, 3.2)}
\begin{thmbox}{Properties}
\begin{itemize}
    \item \textbf{Scaling}: $\det(cA) = c^n \det(A)$.
    \item \textbf{Multiplication}: $\det(AB) = \det(A)\det(B)$.
    \item \textbf{Inverses}: $\det(A^{-1}) = 1/\det(A)$.
    \item \textbf{Row Ops}: Swap ($\times -1$); Scale by $k$ ($\times k$); Replacement (no change).
\end{itemize}
\end{thmbox}

\begin{exbox}{Example: Row Operations}
If $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ has $\det(A) = -2$, swapping rows yields $\det = 2$.
\end{exbox}

\section*{5. Linear Transformations (Sec 1.8, 1.9)}
\begin{algobox}{Mappings}
\begin{itemize}
    \item \textbf{Standard Matrix}: $A = [T(e_1) \dots T(e_n)]$.
    \item \textbf{Onto}: Pivot in every row. Impossible if $m > n$.
    \item \textbf{1-to-1}: Pivot in every column ($Ax=0$ trivial).
    \item \textbf{Rotation}: $R_{\theta} = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}$.
\end{itemize}
\end{algobox}

\section*{6. Detailed Vlamis Style Arguments}
\begin{exbox}{Counter-Examples and Logic}
\begin{itemize}
    \item \textbf{Dependency}: If last col of $AB = \vec{0}$ and $B$ has no zero cols, then $A\vec{b}_n = \vec{0}$ (dependent cols).
    \item \textbf{Identical Cols}: If $\vec{a}_i = \vec{a}_j$, then $A(\vec{e}_i - \vec{e}_j) = \vec{0}$ (not invertible).
    \item \textbf{Subspace Trap}: $W = \{ \begin{bmatrix} x \\ y \end{bmatrix} : xy \ge 0 \}$ fails addition closure.
\end{itemize}
\end{exbox}
\end{multicols}

\newpage
% --- PAGE 3: SUBSPACES & EIGENVALUES ---
\begin{center}
    \textbf{\Large MATH 231 FINAL EXAM BATTLE PLAN - PAGE 3} \\
    \textit{Subspace Properties, Rank-Nullity, and Markov Chains}
\end{center}

\begin{multicols}{2}
\section*{7. Subspaces, Rank \& Basis (Sec 2.8, 2.9)}
\begin{thmbox}{Definitions}
\begin{itemize}
    \item \textbf{Col A}: Basis = Pivot columns of \textbf{original} matrix $A$.
    \item \textbf{Null A}: Basis = vectors in parametric form of $Ax=0$.
    \item \textbf{Rank-Nullity}: $rank(A) + nullity(A) = n$.
\end{itemize}
\end{thmbox}

\section*{8. Eigenvalues ($\lambda$) \& Eigenvectors ($v$)}
\begin{algobox}{Calculations (Sec 5.1-5.3)}
\begin{itemize}
    \item \textbf{Char Equation}: $\det(A - \lambda I) = 0$.
    \item \textbf{Eigenspace ($E_\lambda$)}: $Null(A - \lambda I)$.
    \item \textbf{Diagonalization}: $A = PDP^{-1}$ (requires $n$ independent eigenvectors).
\end{itemize}
\end{algobox}

\begin{exbox}{Example: Eigenvalues}
For $A = \begin{bmatrix} 2 & 3 \\ 0 & 4 \end{bmatrix}$, $\det(A-\lambda I) = (2-\lambda)(4-\lambda) = 0 \Rightarrow \lambda = 2, 4$.
\end{exbox}

\section*{9. Markov Chains (App. 10.1, 10.2)}
\begin{algobox}{Steady-State Calculations}
\begin{itemize}
    \item \textbf{Stochastic Matrix}: Columns $\ge 0$ and sum to 1.
    \item \textbf{Steady-State}: Solve $(M - I)x = \vec{0}$ and normalize entries.
\end{itemize}
\end{algobox}

\begin{exbox}{Example: Steady State}
$M = \begin{bmatrix} 0.7 & 0.2 \\ 0.3 & 0.8 \end{bmatrix}$. Solve $(M-I)x = 0 \Rightarrow \begin{bmatrix} -0.3 & 0.2 \\ 0.3 & -0.2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0$.
$3x_1 = 2x_2$. If $x_1+x_2=1$, then $x_1=0.4, x_2=0.6$. $w = \begin{bmatrix} 0.4 \\ 0.6 \end{bmatrix}$.
\end{exbox}
\end{multicols}

\newpage
% --- PAGE 4: ORTHOGONALITY & LEAST SQUARES ---
\begin{center}
    \textbf{\Large MATH 231 FINAL EXAM BATTLE PLAN - PAGE 4} \\
    \textit{Projections, Gram-Schmidt, QR, and Least Squares}
\end{center}

\begin{multicols}{2}
\section*{10. Orthogonality \& Projections (Sec 6.1-6.3)}
\begin{thmbox}{Theorems}
\begin{itemize}
    \item \textbf{Inner Product}: $u \cdot v = \sum u_i v_i$. Norm $\|v\| = \sqrt{v \cdot v}$.
    \item \textbf{Complement}: $v \in W \cap W^\perp \Rightarrow v = \vec{0}$.
    \item \textbf{Projection}: $\hat{y} = \text{proj}_W y = \sum \frac{y \cdot u_i}{u_i \cdot u_i} u_i$.
    \item \textbf{Decomposition}: $y = \hat{y} + z$, where $z \in W^\perp$.
\end{itemize}
\end{thmbox}

\section*{11. Gram-Schmidt \& QR (Sec 6.4)}
\begin{algobox}{Algorithms}
\begin{itemize}
    \item \textbf{Gram-Schmidt}: $v_1 = x_1$; $v_2 = x_2 - \text{proj}_{v_1} x_2$.
    \item \textbf{QR}: $A = QR$ ($Q$ orthogonal columns, $R$ upper triangular).
\end{itemize}
\end{algobox}

\begin{exbox}{Example: Gram-Schmidt}
$x_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}, x_2 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \Rightarrow v_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$, $v_2 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} - \frac{1}{2}\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0.5 \\ -0.5 \\ 0 \end{bmatrix}$.
\end{exbox}

\section*{12. Least Squares (Sec 6.5, 6.6)}
\begin{algobox}{Optimization}
\begin{itemize}
    \item \textbf{Goal}: Minimize $\|b - Ax\|$.
    \item \textbf{Normal Equations}: Solve $A^T A \hat{x} = A^T b$.
    \item \textbf{Uniqueness}: Unique if columns of $A$ are linearly independent.
\end{itemize}
\end{algobox}
\end{multicols}

\end{document}