\documentclass[9pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.3in]{geometry}
\usepackage{amsmath, amssymb, enumitem, xcolor, multicol}

% Custom section styling
\newcommand{\mysection}[1]{\vspace{4pt}\noindent\textbf{\large\color{blue} #1}\hrule\vspace{2pt}}
\setlist{nosep} 

\begin{document}

% --- PAGE 1: SYSTEMS, MATRIX ALGEBRA & INVERTIBILITY ---
\begin{center}
    \textbf{\Large MATH 231 Final Exam Cheat Sheet - Page 1} \\
    \textit{Focus: Linear Systems, Augmented Matrices, and the Master IMT}
\end{center}

\begin{multicols}{2}
\mysection{1. Linear Systems \& RREF Procedures (Sec 1.1, 1.2)}
\begin{itemize}
    \item \textbf{Augmented Matrix}: For system $Ax=b$, the matrix is $[A \mid b]$.
    \item \textbf{Pivot Position}: A leading 1 in the reduced row echelon form.
    \item \textbf{Variables}: \textbf{Basic} match pivot columns; \textbf{Free} match non-pivot columns.
    \item \textbf{Consistency}: System is inconsistent if the last column of $[A \mid b]$ is a pivot.
    \item \textbf{Solution Counts}: \textbf{Unique} (pivot in every column of $A$); \textbf{Infinite} (consistent with free variables); \textbf{None} (pivot in augmented column).
    \item \textbf{Parametric Solution}: Solve for basic vars in terms of free vars ($s, t$).
    \item \textbf{Example}: $rref(A) = \begin{bmatrix} 1 & -7 & 0 & 2 \\ 0 & 0 & 1 & -1 \\ 0 & 0 & 0 & 0 \end{bmatrix} \Rightarrow x_1 = 7x_2 - 2x_4, x_3 = x_4$.
\end{itemize}

\mysection{2. The Invertible Matrix Theorem (IMT) (Sec 2.3)}
For an $n \times n$ matrix $A$, the following are equivalent:
\begin{itemize}
    \item $A$ is invertible $\iff \det(A) \neq 0$.
    \item $A$ is row equivalent to $I_n$ and has $n$ pivot positions.
    \item $Ax = 0$ has only the trivial solution ($x=0$).
    \item Columns form a basis for $\mathbb{R}^n$ and are linearly independent.
    \item $x \mapsto Ax$ is one-to-one and onto $\mathbb{R}^n$.
    \item $rank(A) = n$ and $nullity(A) = 0$.
\end{itemize}

\mysection{3. Matrix Algebra \& Inverse Computation (Sec 2.1, 2.2)}
\begin{itemize}
    \item \textbf{Inverses}: $(AB)^{-1} = B^{-1}A^{-1}$.
    \item \textbf{Algorithm}: $[A \mid I] \sim [I \mid A^{-1}]$ via row ops.
    \item \textbf{2x2 Shortcut}: $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \Rightarrow A^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$.
    \item \textbf{Elementary Matrices ($E$)}: Row op on $I$. $A \to B \Rightarrow B=EA$.
    \item \textbf{Logic}: If $AB$ and $B$ are invertible, $A = (AB)B^{-1}$ is invertible.
\end{itemize}
\end{multicols}

\newpage
% --- PAGE 2: DETERMINANTS & TRANSFORMATIONS ---
\begin{center}
    \textbf{\Large MATH 231 Final Exam Cheat Sheet - Page 2} \\
    \textit{Focus: Determinant Algebra, Transformations, and Vlamis Logic Arguments}
\end{center}

\begin{multicols}{2}
\mysection{4. Determinant Operations \& Properties (Sec 3.1, 3.2)}
\begin{itemize}
    \item \textbf{Calculations}: Expand along row/col with most zeros.
    \item \textbf{Scaling}: $\det(cA) = c^n \det(A)$ for $n \times n$ matrix.
    \item \textbf{Multiplication}: $\det(AB) = \det(A)\det(B)$.
    \item \textbf{Inverses}: $\det(A^{-1}) = 1/\det(A)$.
    \item \textbf{Row Ops}: Swap rows ($\times -1$); Scale row by $k$ ($\times k$); Replacement (no change).
\end{itemize}

\mysection{5. Linear Transformations (Sec 1.8, 1.9)}
\begin{itemize}
    \item \textbf{Standard Matrix}: $A = [T(e_1) \dots T(e_n)]$.
    \item \textbf{Onto}: Pivot in every row. Impossible if $m > n$.
    \item \textbf{1-to-1}: $Ax=0$ has only trivial sol (pivot in every column).
    \item \textbf{Geometric Matrix}: $R_{\theta} = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}$ is an orthogonal rotation matrix.
\end{itemize}

\mysection{6. Detailed Vlamis Style Arguments (Counters)}
\begin{itemize}
    \item \textbf{Dependency Arg}: If last column of $AB$ is $\vec{0}$ and $B$ has no zero columns, then $A\vec{b}_n = \vec{0}$, so columns of $A$ are dependent.
    \item \textbf{Identical Cols}: If square $A$ has identical cols $\vec{a}_i = \vec{a}_j$, then $A(\vec{e}_i - \vec{e}_j) = \vec{0}$, so $A$ is not invertible.
    \item \textbf{Subspace Counter}: $W = \{ \begin{bmatrix} x \\ y \end{bmatrix} : xy \ge 0 \}$ is NOT a subspace (fails addition closure).
\end{itemize}
\end{multicols}

\newpage
% --- PAGE 3: SUBSPACES & EIGENVALUES ---
\begin{center}
    \textbf{\Large MATH 231 Final Exam Cheat Sheet - Page 3} \\
    \textit{Focus: Subspace Properties, Rank-Nullity, and Eigenvalue Calculations}
\end{center}

\begin{multicols}{2}
\mysection{7. Subspaces, Rank \& Basis (Sec 2.8, 2.9)}
\begin{itemize}
    \item \textbf{Col A}: Span of columns. Basis = Pivot columns of \textbf{original} matrix $A$.
    \item \textbf{Null A}: Solutions to $Ax = 0$. Basis = vectors in parametric form.
    \item \textbf{Rank-Nullity}: $rank(A) + nullity(A) = n$ (total cols).
    \item \textbf{Dimension}: If $\dim(W) = \dim(V)$ and $W \subset V$, then $W = V$.
\end{itemize}

\mysection{8. Eigenvalues ($\lambda$) \& Eigenvectors ($v$) (Sec 5.1-5.3)}
\begin{itemize}
    \item \textbf{Check Eigenvector}: $v \neq \vec{0}$ is eigenvector if $Av = \lambda v$.
    \item \textbf{Char Equation}: $\det(A - \lambda I) = 0$.
    \item \textbf{Eigenspace ($E_\lambda$)}: $Null(A - \lambda I)$.
    \item \textbf{Diagonalization}: $A = PDP^{-1}$ if $A$ has $n$ independent eigenvectors.
\end{itemize}

\mysection{9. Markov Chains (Appendix 10.1, 10.2)}
\begin{itemize}
    \item \textbf{Stochastic Matrix}: Columns $\ge 0$ and sum to 1.
    \item \textbf{Steady-State ($w$)}: Solve $(M - I)x = \vec{0}$ and normalize entries to sum to 1.
    \item \textbf{Graph Transition}: $M_{ij}$ is prob from state $j$ to state $i$.
\end{itemize}
\end{multicols}

\newpage
% --- PAGE 4: ORTHOGONALITY & LEAST SQUARES ---
\begin{center}
    \textbf{\Large MATH 231 Final Exam Cheat Sheet - Page 4} \\
    \textit{Focus: Projections, Gram-Schmidt, QR, and Least Squares}
\end{center}

\begin{multicols}{2}
\mysection{10. Orthogonality \& Projections (Sec 6.1-6.3)}
\begin{itemize}
    \item \textbf{Inner Product}: $u \cdot v = \sum u_i v_i$. Norm $\|v\| = \sqrt{v \cdot v}$.
    \item \textbf{Complement ($W^\perp$)}: $v \cdot w = 0$ for all $w \in W$.
    \item \textbf{Theorem}: $v \in W \cap W^\perp \Rightarrow v = \vec{0}$.
    \item \textbf{Projection}: $\hat{y} = \text{proj}_W y = \sum \frac{y \cdot u_i}{u_i \cdot u_i} u_i$.
    \item \textbf{Decomposition}: $y = \hat{y} + z$, where $z = y - \hat{y} \in W^\perp$.
\end{itemize}

\mysection{11. Algorithms: Gram-Schmidt \& QR (Sec 6.4)}
\begin{itemize}
    \item \textbf{Gram-Schmidt}: Converts basis $\{x_i\}$ to orthogonal basis $\{v_i\}$:
        1. $v_1 = x_1$
        2. $v_2 = x_2 - \text{proj}_{v_1} x_2$
        3. $v_3 = x_3 - \text{proj}_{v_1} x_3 - \text{proj}_{v_2} x_3$.
    \item \textbf{QR Decomposition}: $A = QR$ ($Q$ orthogonal columns, $R$ upper triangular).
\end{itemize}

\mysection{12. Least Squares (Sec 6.5, 6.6)}
\begin{itemize}
    \item \textbf{Definition}: Find $\hat{x}$ that minimizes $\|b - Ax\|$.
    \item \textbf{Normal Equations}: Solve $A^T A \hat{x} = A^T b$.
    \item \textbf{Uniqueness}: Unique if columns of $A$ are lin. independent.
\end{itemize}
\end{multicols}

\end{document}